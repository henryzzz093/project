{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_img, train_labels), (test_img, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_img))\n",
    "print(len(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_img[0])\n",
    "print(train_labels[0])\n",
    "# print(train_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing dataset\n",
    "train_img = train_img / 255.0\n",
    "test_img = test_img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28, 28)),  # first imput is 28 x 28, which is the size of image\n",
    "    keras.layers.Dense(128, activation = 'relu'),  # \n",
    "    keras.layers.Dense(10, activation = 'softmax') # 10 - the number of different items of clothing represented in dataset\n",
    "]) \n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5031 - accuracy: 0.8212\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3791 - accuracy: 0.8635\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3412 - accuracy: 0.8761\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3131 - accuracy: 0.8857\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2958 - accuracy: 0.8903\n",
      "313/313 [==============================] - 0s 820us/step - loss: 0.3397 - accuracy: 0.8793\n",
      "0.3397398293018341\n",
      "0.8792999982833862\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_img, train_labels, epochs = 5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_img, test_labels)\n",
    "print(test_loss)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1830946e-06 4.3121737e-09 6.0258259e-08 1.0324401e-06 3.9572335e-08\n",
      " 2.9463405e-03 7.3906295e-07 3.0937992e-02 2.2910949e-06 9.6611029e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_img)\n",
    "print(classifications[0]) # show the probability that this item is each of the 10 classes\n",
    "print(test_labels[0]) # as 10th element being the highest value, label 9 means that it is the 10th of the 10 classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4727\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3580\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3208\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2950\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2774\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3296\n",
      "[5.6178663e-08 6.5455121e-09 3.1836942e-09 2.1895831e-10 6.2626238e-10\n",
      " 2.0036280e-04 9.9513786e-10 1.5946241e-03 2.6104969e-09 9.9820495e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# now we increase the value of the dense layer to 512 neurons, what different results do we get for loss, training time?\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(train_img, train_labels, epochs = 5)\n",
    "\n",
    "model.evaluate(test_img, test_labels)\n",
    "classifications = model.predict(test_img)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from here we can see that the training takes longer, but it more accurate, but this doesnot means more is better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the final layers, why are there 10 of them? because we have 10 different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4704\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3590\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3215\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2999\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2802\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2655\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2540\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2408\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2303\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2209\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2123\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2056\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1967\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1928\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1876\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4441\n",
      "[4.9359680e-15 2.3089691e-17 8.1329999e-17 3.8112037e-18 1.2912875e-17\n",
      " 2.8050609e-13 3.7691632e-16 1.1334767e-02 2.7593479e-15 9.8866522e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# change epochs to 15:\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(train_img, train_labels, epochs = 15)\n",
    "\n",
    "model.evaluate(test_img, test_labels)\n",
    "classifications = model.predict(test_img)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4736 - accuracy: 0.8300\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3574 - accuracy: 0.8686\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3236 - accuracy: 0.8809\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2948 - accuracy: 0.8919\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2778 - accuracy: 0.8972\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2629 - accuracy: 0.9006\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2488 - accuracy: 0.9069\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2388 - accuracy: 0.9113\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2299 - accuracy: 0.9136\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2190 - accuracy: 0.9178\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2103 - accuracy: 0.9199\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2014 - accuracy: 0.9226\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1942 - accuracy: 0.9267\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1881 - accuracy: 0.9289\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1825 - accuracy: 0.9303\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1787 - accuracy: 0.9332\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1720 - accuracy: 0.9349\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1648 - accuracy: 0.9374\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1623 - accuracy: 0.9394\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1557 - accuracy: 0.9409\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1513 - accuracy: 0.9432\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1470 - accuracy: 0.9445\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1437 - accuracy: 0.9447\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1403 - accuracy: 0.9460\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1341 - accuracy: 0.9488\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1321 - accuracy: 0.9502\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1271 - accuracy: 0.9517\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1246 - accuracy: 0.9536\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1220 - accuracy: 0.9540\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1217 - accuracy: 0.9546\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4185 - accuracy: 0.8988\n",
      "[6.7433458e-16 1.6453215e-21 2.8604739e-20 1.2059558e-25 7.5768167e-20\n",
      " 5.3445373e-09 6.9010830e-21 1.1793476e-06 4.1800038e-22 9.9999881e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# change epochs to 15:\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_img, train_labels, epochs = 30)\n",
    "\n",
    "model.evaluate(test_img, test_labels)\n",
    "classifications = model.predict(test_img)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, improve computer vision accuracy with convolutions\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_img, train_labels), (test_img, test_labels) = fashion_mnist.load_data()\n",
    "# training data needed to be reshaped, this is because the first convolution espects a single tensor containing everything\n",
    "# hence instead of 60000, 28 * 28 * 1 items in a list, we need to have a single 4D list that is 60000 x 28 x 28 x 1\n",
    "train_img = train_img.reshape(60000,28,28,1)\n",
    "train_img = train_img / 255.0\n",
    "test_img = test_img.reshape(10000,28,28,1)\n",
    "test_img = test_img / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.4468 - accuracy: 0.8352\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.2960 - accuracy: 0.8920\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.2485 - accuracy: 0.9080\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.2167 - accuracy: 0.9197\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.1900 - accuracy: 0.9288\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4185 - accuracy: 0.8988\n",
      "Test loss: 0.4185369610786438, Test accuracy: 89.88000154495239\n"
     ]
    }
   ],
   "source": [
    "con_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu', input_shape = (28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Add another convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # flatten the output, after this you will have the same DNN as the non convolution version\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "con_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "con_model.summary()\n",
    "con_model.fit(train_img, train_labels, epochs = 5)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_img, test_labels)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3851624e-07 1.4021710e-07 2.3192219e-07 5.8438050e-09 2.6232383e-10\n",
      " 1.9280542e-06 4.3782713e-08 4.5105262e-05 5.9106860e-08 9.9995244e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "classifications = con_model.predict(test_img)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
